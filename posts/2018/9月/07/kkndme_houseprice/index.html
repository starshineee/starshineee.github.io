<!doctype html>
<html lang="chinese (simplified)">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>天涯 kkndme 大神论房价 | Starshineee 的个人博客
</title>
  <link rel="canonical" href="https://starshineee.github.io/posts/2018/9月/07/kkndme_houseprice/index.html">


  <link rel="stylesheet" href="https://starshineee.github.io/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://starshineee.github.io/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://starshineee.github.io/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://starshineee.github.io/theme/css/theme.css">


<meta name="description" content="天涯神贴不解释，由本人利用 python 爬虫爬得并整理。本文提供整理后文档的下载，并对爬虫进行了简述。">
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-124959139-1', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://starshineee.github.io/">Starshineee 的个人博客</a></h1>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="http://getpelican.com/" target="_blank">Pelican</a></li>
          <li class="list-inline-item"><a href="http://python.org/" target="_blank">Python.org</a></li>
          <li class="list-inline-item"><a href="http://jinja.pocoo.org/" target="_blank">Jinja2</a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>天涯 kkndme 大神论房价
</h1>
      <hr>
<article class="article">
  <header>
    <ul class="list-inline">
      <li class="list-inline-item text-muted" title="2018-09-07T23:24:10.032029+08:00">
        <i class="fa fa-clock-o"></i>
        周五 07 九月 2018
      </li>
      <li class="list-inline-item">
        <i class="fa fa-folder-open-o"></i>
        <a href="https://starshineee.github.io/category/pa-chong-fang-jie.html">爬虫, 房价</a>
      </li>
      <li class="list-inline-item">
        <i class="fa fa-user-o"></i>
        <a href="https://starshineee.github.io/author/starshineee.html">starshineee</a>      </li>
    </ul>
  </header>
  <div class="content">
    <h1>文档简介及下载链接</h1>
<p>2010年天涯神贴，主要内容为 kkndme 大神对于房价、经济、历史等热点的看法。由于其准确预言了中国的经济和房价的发展历程，使得其在 2018 年被突然发现并迅速变为热帖。由于很多观点有着一定的敏感性，所以多处面临着删帖的风险。本人利用 python 爬虫将其整理成了一个文档并提供下载，以方便自己和大家阅读。遗憾的是本人下手太晚，一些楼层已经被删贴了，但剩下的那部分依然能够反映出 kkndme 大神的观点，读之如饮醇醪。</p>
<p>下载链接：
<a href="https://starshineee.github.io/pdfs/天涯kkndme神贴论房价.pdf">pdf 版 kkndme 论房价</a></p>
<h1>爬虫思路</h1>
<p>爬虫的本质是利用计算机来模拟人访问页面的行为，所以我们首先要想我们自己访问天涯神贴的时候怎么操作的。事实上，我们有两种途径：一种是访问<a href="http://bbs.tianya.cn/post-5137-9507-1.shtml">PC版的界面</a>，一种是访问<a href="http://bbs.tianya.cn/m/post-house-252774-1.shtml">移动端的界面</a>。这两者中移动端的界面较为清爽，且可以通过点击“只看楼主”来滤掉大部分的非目标信息，对应的链接就变成了<a href="http://bbs.tianya.cn/m/post_author-house-252774-1.shtml">移动端只看楼主</a>，这样便大大地方便了我们的爬取。本文中代码默认需要使用者有库 requests, bs4, python-docx, time。</p>
<p>首先我们选择一个header并以此来对上述链接进行访问</p>
<div class="highlight"><pre><span></span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;http://bbs.tianya.cn/m/post_author-house-252774-1.shtml&#39;</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span><span class="s1">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#39;</span>
    <span class="p">}</span>
<span class="n">r</span><span class="o">=</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="o">+</span><span class="n">page</span><span class="p">,</span><span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">)</span>
<span class="n">soup</span><span class="o">=</span><span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">,</span><span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>
</pre></div>


<p>打印soup，我们可以看到其中已经包含了我们想要得到的网页中的一切信息，现在需要把我们感兴趣的信息提取出来，即kkndme大神的发帖内容，以及去往下一页的链接。</p>
<p>观察页面内容，发现我们感兴趣的帖子内容都在 "class=bd" 的 "div" 元素中，且都在首层的 "p" 元素中，因此可以通过下面代码得到每一段的内容。</p>
<div class="highlight"><pre><span></span><span class="n">contents</span><span class="o">=</span><span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span><span class="n">class_</span><span class="o">=</span><span class="s1">&#39;bd&#39;</span><span class="p">)</span>
<span class="n">paragraphs</span><span class="o">=</span><span class="n">content</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span><span class="n">recursive</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paragraphs</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_text</span><span class="p">())</span>
</pre></div>


<p>在 "class=u-btn next-btn" 的 "a" 元素中我们能找到向下一页跳转的链接，这样我们就可以自动地去访问下一个页面。</p>
<div class="highlight"><pre><span></span><span class="n">next_page</span><span class="o">=</span><span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="n">class_</span><span class="o">=</span><span class="s1">&#39;u-btn next-btn&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">)</span>
</pre></div>


<p>看起来好像我们已经得到了我们想要的，然而访问几个页面后就会出现问题。。。这是因为天涯允许匿名访客访问限制数目的页面，但再往后就要登录才能访问。这个时候我们就需要先用自己的账号登录，如下图所示在 chrome 中利用 F12 查看自己的 cookie，处理后填到 requests 中。</p>
<p><img alt="获取cookie" height="400px" src="https://starshineee.github.io/images/获取cookie.png" width="600px"></p>
<p>最后利用 python-docx 包将帖子内容存进 word 文档中，本文中所提供的 pdf 即由该 word 转化而来。</p>
<h1>最终代码</h1>
<p>爬取页面的大致思路如上段所述，具体的代码要复杂一些，如下所示</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span><span class="nn">time</span>
<span class="kn">from</span> <span class="nn">docx</span> <span class="kn">import</span> <span class="n">Document</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>

<span class="n">base_url</span><span class="o">=</span><span class="s1">&#39;http://bbs.tianya.cn/m/&#39;</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span><span class="s1">&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#39;</span>
    <span class="p">}</span>
<span class="n">layer_ind</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">page</span> <span class="o">=</span> <span class="s1">&#39;post_author-house-252774-1.shtml&#39;</span>

<span class="n">cookie</span> <span class="o">=</span> <span class="p">{</span><span class="n">yourcookie</span><span class="p">}</span> <span class="c1"># 自己登录天涯并从浏览器中得到 yourcookie</span>
<span class="n">cookies</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">cookie</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;;&#39;</span><span class="p">):</span>
    <span class="n">name</span><span class="p">,</span><span class="n">value</span><span class="o">=</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;=&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cookies</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">=</span><span class="n">value</span>


<span class="n">document</span> <span class="o">=</span> <span class="n">Document</span><span class="p">()</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">page</span><span class="p">)</span>
    <span class="n">r</span><span class="o">=</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">base_url</span><span class="o">+</span><span class="n">page</span><span class="p">,</span><span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span><span class="n">cookies</span><span class="o">=</span><span class="n">cookies</span><span class="p">)</span>
    <span class="n">soup</span><span class="o">=</span><span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">,</span><span class="s1">&#39;html.parser&#39;</span><span class="p">)</span>
    <span class="n">contents</span><span class="o">=</span><span class="n">soup</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s1">&#39;div&#39;</span><span class="p">,</span><span class="n">class_</span><span class="o">=</span><span class="s1">&#39;bd&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">contents</span><span class="p">:</span>
        <span class="n">paragraphs</span><span class="o">=</span><span class="n">content</span><span class="o">.</span><span class="n">find_all</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span><span class="n">recursive</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">document</span><span class="o">.</span><span class="n">add_heading</span><span class="p">(</span><span class="s1">&#39;层{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer_ind</span><span class="p">),</span><span class="n">level</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paragraphs</span><span class="p">:</span>
            <span class="n">document</span><span class="o">.</span><span class="n">add_paragraph</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get_text</span><span class="p">())</span>
        <span class="n">document</span><span class="o">.</span><span class="n">add_paragraph</span><span class="p">(</span><span class="s1">&#39;==============================&#39;</span><span class="p">)</span>
        <span class="n">layer_ind</span><span class="o">+=</span><span class="mi">1</span>

    <span class="n">page</span><span class="o">=</span><span class="n">soup</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="n">class_</span><span class="o">=</span><span class="s1">&#39;u-btn next-btn&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">page</span><span class="o">==</span><span class="bp">None</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">page</span><span class="o">=</span><span class="n">page</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;href&#39;</span><span class="p">)</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">document</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;result.docx&#39;</span><span class="p">)</span>
</pre></div>
  </div>

  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script>
    !function(n){"use strict";function t(n,t){var r=(65535&n)+(65535&t);return(n>>16)+(t>>16)+(r>>16)<<16|65535&r}function r(n,t){return n<<t|n>>>32-t}function e(n,e,o,u,c,f){return t(r(t(t(e,n),t(u,f)),c),o)}function o(n,t,r,o,u,c,f){return e(t&r|~t&o,n,t,u,c,f)}function u(n,t,r,o,u,c,f){return e(t&o|r&~o,n,t,u,c,f)}function c(n,t,r,o,u,c,f){return e(t^r^o,n,t,u,c,f)}function f(n,t,r,o,u,c,f){return e(r^(t|~o),n,t,u,c,f)}function i(n,r){n[r>>5]|=128<<r%32,n[14+(r+64>>>9<<4)]=r;var e,i,a,d,h,l=1732584193,g=-271733879,v=-1732584194,m=271733878;for(e=0;e<n.length;e+=16)i=l,a=g,d=v,h=m,g=f(g=f(g=f(g=f(g=c(g=c(g=c(g=c(g=u(g=u(g=u(g=u(g=o(g=o(g=o(g=o(g,v=o(v,m=o(m,l=o(l,g,v,m,n[e],7,-680876936),g,v,n[e+1],12,-389564586),l,g,n[e+2],17,606105819),m,l,n[e+3],22,-1044525330),v=o(v,m=o(m,l=o(l,g,v,m,n[e+4],7,-176418897),g,v,n[e+5],12,1200080426),l,g,n[e+6],17,-1473231341),m,l,n[e+7],22,-45705983),v=o(v,m=o(m,l=o(l,g,v,m,n[e+8],7,1770035416),g,v,n[e+9],12,-1958414417),l,g,n[e+10],17,-42063),m,l,n[e+11],22,-1990404162),v=o(v,m=o(m,l=o(l,g,v,m,n[e+12],7,1804603682),g,v,n[e+13],12,-40341101),l,g,n[e+14],17,-1502002290),m,l,n[e+15],22,1236535329),v=u(v,m=u(m,l=u(l,g,v,m,n[e+1],5,-165796510),g,v,n[e+6],9,-1069501632),l,g,n[e+11],14,643717713),m,l,n[e],20,-373897302),v=u(v,m=u(m,l=u(l,g,v,m,n[e+5],5,-701558691),g,v,n[e+10],9,38016083),l,g,n[e+15],14,-660478335),m,l,n[e+4],20,-405537848),v=u(v,m=u(m,l=u(l,g,v,m,n[e+9],5,568446438),g,v,n[e+14],9,-1019803690),l,g,n[e+3],14,-187363961),m,l,n[e+8],20,1163531501),v=u(v,m=u(m,l=u(l,g,v,m,n[e+13],5,-1444681467),g,v,n[e+2],9,-51403784),l,g,n[e+7],14,1735328473),m,l,n[e+12],20,-1926607734),v=c(v,m=c(m,l=c(l,g,v,m,n[e+5],4,-378558),g,v,n[e+8],11,-2022574463),l,g,n[e+11],16,1839030562),m,l,n[e+14],23,-35309556),v=c(v,m=c(m,l=c(l,g,v,m,n[e+1],4,-1530992060),g,v,n[e+4],11,1272893353),l,g,n[e+7],16,-155497632),m,l,n[e+10],23,-1094730640),v=c(v,m=c(m,l=c(l,g,v,m,n[e+13],4,681279174),g,v,n[e],11,-358537222),l,g,n[e+3],16,-722521979),m,l,n[e+6],23,76029189),v=c(v,m=c(m,l=c(l,g,v,m,n[e+9],4,-640364487),g,v,n[e+12],11,-421815835),l,g,n[e+15],16,530742520),m,l,n[e+2],23,-995338651),v=f(v,m=f(m,l=f(l,g,v,m,n[e],6,-198630844),g,v,n[e+7],10,1126891415),l,g,n[e+14],15,-1416354905),m,l,n[e+5],21,-57434055),v=f(v,m=f(m,l=f(l,g,v,m,n[e+12],6,1700485571),g,v,n[e+3],10,-1894986606),l,g,n[e+10],15,-1051523),m,l,n[e+1],21,-2054922799),v=f(v,m=f(m,l=f(l,g,v,m,n[e+8],6,1873313359),g,v,n[e+15],10,-30611744),l,g,n[e+6],15,-1560198380),m,l,n[e+13],21,1309151649),v=f(v,m=f(m,l=f(l,g,v,m,n[e+4],6,-145523070),g,v,n[e+11],10,-1120210379),l,g,n[e+2],15,718787259),m,l,n[e+9],21,-343485551),l=t(l,i),g=t(g,a),v=t(v,d),m=t(m,h);return[l,g,v,m]}function a(n){var t,r="",e=32*n.length;for(t=0;t<e;t+=8)r+=String.fromCharCode(n[t>>5]>>>t%32&255);return r}function d(n){var t,r=[];for(r[(n.length>>2)-1]=void 0,t=0;t<r.length;t+=1)r[t]=0;var e=8*n.length;for(t=0;t<e;t+=8)r[t>>5]|=(255&n.charCodeAt(t/8))<<t%32;return r}function h(n){return a(i(d(n),8*n.length))}function l(n,t){var r,e,o=d(n),u=[],c=[];for(u[15]=c[15]=void 0,o.length>16&&(o=i(o,8*n.length)),r=0;r<16;r+=1)u[r]=909522486^o[r],c[r]=1549556828^o[r];return e=i(u.concat(d(t)),512+8*t.length),a(i(c.concat(e),640))}function g(n){var t,r,e="";for(r=0;r<n.length;r+=1)t=n.charCodeAt(r),e+="0123456789abcdef".charAt(t>>>4&15)+"0123456789abcdef".charAt(15&t);return e}function v(n){return unescape(encodeURIComponent(n))}function m(n){return h(v(n))}function p(n){return g(m(n))}function s(n,t){return l(v(n),v(t))}function C(n,t){return g(s(n,t))}function A(n,t,r){return t?r?s(t,n):C(t,n):r?m(n):p(n)}"function"==typeof define&&define.amd?define(function(){return A}):"object"==typeof module&&module.exports?module.exports=A:n.md5=A}(this);

  </script>
  <script>
    var gitalk = new Gitalk({
      clientID: '83ce4a7931514df054dc',
      clientSecret: 'eedf908158dfdfd50b796349175b1ce0384f662c',
      repo: 'starshineee.github.io',
      owner: 'starshineee',
      admin: ['starshineee'],
      id: md5(location.pathname),
      distractionFreeMode: false
    });
    gitalk.render('gitalk-container');
  </script>


</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
      <li class="list-inline-item"><a href="https://starshineee.github.io/authors.html">Authors</a></li>
    <li class="list-inline-item"><a href="https://starshineee.github.io/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="https://starshineee.github.io/categories.html">Categories</a></li>
  </ul>
  <!-- <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p> -->
</div>    </div>
  </footer>
</body>

</html>